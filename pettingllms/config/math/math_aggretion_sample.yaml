defaults:
- ../ppo_trainer@models.model_0.ppo_trainer_config: eval
- _self_
specialization: prompt
resource:
  nnodes: 1
  n_gpus_per_node: 8
  trust_remote_code: true
lora_rank: 16
lora_alpha: 32
env:
  name: math_aggretion_env
  dataset: polaris
  benchmark: AIME24
  max_turns: 3
  resolve: false
  multi_modal: false
  batched_init: true
base_models:
  policy_0:
    path: your base model path
    name: shared_model
agent_policy_configs:
  agent_configs:
    agent_0:
      name: sample_reasoning_agent
      policy_name: shared_model
      sample_num: 3
      train_llm_config:
        enable_thinking: true
        temperature: 1.0
        top_p: 0.9
        top_k: 20
        min_p: 0.0
      val_llm_config:
        enable_thinking: true
        temperature: 0.7
        top_p: 0.95
        top_k: 20
        min_p: 0.0
    agent_1:
      name: sample_tool_agent
      policy_name: shared_model
      sample_num: 3
      train_llm_config:
        enable_thinking: true
        temperature: 1.0
        top_p: 0.9
        top_k: 20
        min_p: 0.0
      val_llm_config:
        enable_thinking: true
        temperature: 0.7
        top_p: 0.95
        top_k: 20
        min_p: 0.0
    agent_2:
      name: aggregation_agent
      policy_name: shared_model
      sample_num: 1
      train_llm_config:
        enable_thinking: true
        temperature: 0.7
        top_p: 0.9
        top_k: 20
        min_p: 0.0
      val_llm_config:
        enable_thinking: true
        temperature: 0.6
        top_p: 0.95
        top_k: 20
        min_p: 0.0
multi_agent_interaction:
  turn_order:
  - sample_reasoning_agent
  - sample_tool_agent
  - aggregation_agent
  parallel: true
training:
  total_epochs: 1
  num_workers: 1800
  experiment_name: math_sampling_experiment
  train_batch_size: 16
  train_sample_num: 1
  validate_sample_num: 1
  max_prompt_length: 1024
  max_response_length: 2048
  generate_timeout: 300.0
  step_timeout: 30.0
models:
  model_0:
    model_name: shared_model
    tokenizer_name: shared_model
