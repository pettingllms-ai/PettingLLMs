
defaults:
  - ../ppo_trainer@models.model_0.ppo_trainer_config: eval
  - _self_


specialization: "prompt" # prompt, lora, full
resource:
  nnodes: 1
  n_gpus_per_node: 8
  trust_remote_code: true
lora_rank: 16
lora_alpha: 32

# Environment configuration (multi-agent)
env:
  name: pychecker_rl_env
  dataset: "pychecker"
  benchmark: "pychecker"
  max_turns: 1  # Turn 0: gen_tb_agent, Turn 1: pychecker_agent, Turn 2: optional refinement
  resolve: false
  multi_modal: false
  batched_init: true


base_models:
  policy_0:
    path: "your base model path"
    name: "shared_model"
  

# Multi-agent configuration (2 agents)
agent_policy_configs:
  # Number of agents to train
  num_agents: 2
  policy_list: ["gen_tb_agent", "pychecker_agent"]
  agent_configs:
    agent_0:
      name: "gen_tb_agent"
      policy_name: "shared_model"
      enable_thinking: true
      train_temperature: 1
      val_temperature: 0

    agent_1:
      name: "pychecker_agent"
      policy_name: "shared_model"
      enable_thinking: true
      train_temperature: 1
      val_temperature: 0

      

# Multi-agent interaction configuration
multi_agent_interaction:
  
  # Turn order for agents (list of agent names)
  turn_order: ["gen_tb_agent", "pychecker_agent"]
  
  # Number of agents that interact per episode
  num_interacting_agents: 2
      



training:
  device: cuda
  total_training_steps: 200
  project_name: pettingllms
  experiment_name: pychecker_rl_multi_agent
  logger: [ 'console', 'wandb' ]
  model_checkpoints_dir: checkpoints
  ray_wait_register_center_timeout: 300
  train_batch_size: 32
  train_sample_num: 8
  validate_sample_num: 1
  sample_temperature: 1
  val_freq: 10
  resample_freq: 3
  max_prompt_length: 8192
  max_response_length: 4096
  lora_rank: ${lora_rank}
  lora_alpha: ${lora_alpha}
  num_workers: 512  # Number of Ray workers for parallel task execution
  step_timeout: 180.0  # Timeout for each agent step (Python + Verilog simulation)
  generate_timeout: 300.0  # Timeout for LLM generation


trainer:
  device: cuda
  n_gpus_per_node: ${resource.n_gpus_per_node}
  nnodes: 1
  balance_batch: true
  total_epochs: 1
  total_training_steps: ${training.total_training_steps}
  project_name: ${training.project_name}
  experiment_name: ${training.experiment_name}
  logger: ${training.logger}
  default_local_dir: checkpoints
  ray_wait_register_center_timeout: ${training.ray_wait_register_center_timeout}
  n_training_gpus_per_node: ${resource.n_gpus_per_node}


models:
  model_0:
    path: ${base_models.policy_0.path}
    name: ${base_models.policy_0.name}
    ppo_trainer_config:
      filter_method: dapo
      filter_ratio: 0.3
      data: 
        max_prompt_length: ${training.max_prompt_length}
        max_response_length: ${training.max_response_length}
      actor_rollout_ref:
        model:
          path: ${base_models.policy_0.path}
        rollout:
          temperature: ${training.sample_temperature}
          prompt_length: ${training.max_prompt_length}
          response_length: ${training.max_response_length}
          tensor_model_parallel_size: ${resource.n_gpus_per_node}
        trainer:
          n_gpus_per_node: ${resource.n_gpus_per_node}
          n_training_gpus_per_node: ${resource.n_gpus_per_node}
          default_local_dir: ${trainer.default_local_dir}

