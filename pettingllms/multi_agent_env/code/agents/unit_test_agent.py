import copy
import logging
from typing import Any

from pettingllms.multi_agent_env.base.agent import Agent, AgentData
from pettingllms.multi_agent_env.base.env import Env
from pettingllms.utils.logger_config import get_multi_logger
from pettingllms.multi_agent_env.code.code_utils import extract_test_cases

logger = logging.getLogger(__name__)


def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[: length // 2] + "...(truncated) ..." + s[-length // 2 :]


class UnitTestGenerationAgent(Agent):
    """
    Agent specialized for generating unit test cases.
    """

    def __init__(self, rollout_idx: int | None = None, **kwargs):
        """
        Initialize the Unit Test Generation Agent.
        """
        super().__init__()
        self.rollout_idx = rollout_idx
        # Accept other unrelated keyword arguments for compatibility
        for key, value in (kwargs or {}).items():
            setattr(self, key, value)
        
        # 初始化多日志系统
        self.multi_logger = get_multi_logger()

    def update_from_env(self, env_data: Env):
        """
        Update the agent's internal prompt after an environment step.
        Rules:
        - If either state.current_code or state.current_test_input is None/empty, prompt to generate test cases.
        - Otherwise, refine or correct tests based on existing code and test cases.
        """
        # Save environment data
        self.env_data = env_data

        state = getattr(env_data, "state", None)
        agent_obs = getattr(env_data, "agent_observations", None)

        def as_text(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, list):
                return "\n".join([str(v) for v in value])
            return str(value)


        question = getattr(state, "problem", None)
        current_code = getattr(state, "generated_code", None)
        current_test_input = getattr(state, "generated_test_input", None)
        current_test_output = getattr(state, "generated_test_output", None)
        current_code_output = getattr(state, "exe_code_generated_test_output", None)
        need_generate = current_code in (None, "") or current_test_input in (None, "") or current_test_output in (None, "") or current_code_output in (None, "")


        if need_generate:
            # Test-case generation mode
            formatted_prompt = (
                f" You are a helpful assistant that generates test examples for coding tasks.  \n"
                f" User: Given a coding task, instead of providing the final script, your task is to generate a new test example (both input, output and explanation).\n"
                f"This is the problem:\n{question}\n\n"
                f"You need to provide a new test example. A good test example should be completely accurate and conform to the problem's format requirements, while also possessing enough discriminative power to distinguish correct code from incorrect code.\n"
                f"Before providing a test example, you must think carefully and reason step by step to derive an input and output you are very confident are correct. For example, start by designing an input you can reliably handle, then compute the output step by step. If you're unsure about the output, revise or re-design the input to ensure accuracy. Directly providing input/output pairs without this process is discouraged, as it often results in low accuracy.\n"
                f"Finally, after completing these previous thinking and derivation steps (you should not write the final test example unless you have gone through these steps very thoroughly), you MUST put your final test example in the following format:\n\n"
                f"**Test Input:**\n```\ninput here\n```\n\n"
                f"**Test Output:**\n```\noutput here\n```\n\n"
                f"**Explanation:**\nexplanation here.  \n"
                f" Assistant:"
            )
        else:
            # Test-case refinement mode
            formatted_prompt = (
                f" You are a helpful assistant that refines or corrects test examples for coding tasks.  \n"
                f" User: Given a coding task, instead of providing the final script, your task is to refine or correct test examples.\n"
                f"This is the problem:\n{question}\n\n"
                f"Problem:\n{question}\n\n"
                f"Current code:\n{as_text(current_code)}\n\n"
                f"but the execution result is not aligned with the test case outputs generated by another test case generator.\n"
                f"Current generated test cases (inputs):\n{as_text(current_test_input)}\n\n"
                f"Current generated test cases (outputs):\n{as_text(current_test_output)}\n\n"
                f"Current code execution result:\n{as_text(current_code_output)}\n\n"
                f"First, you need to judge the mismatch between the current generated test cases and the current code execution result, if the mismatch is caused by the current generated test cases, please refine the test cases to pass all tests.\n"
                f"Then, you need to refine the code to pass all tests.\n"
                f"Finally, you MUST put your final test example in the following format:\n\n"
                f"**Test Input:**\n```\ninput here\n```\n\n"
                f"**Test Output:**\n```\noutput here\n```\n\n"
            )

        self.current_prompt = {"text": formatted_prompt, "image": None}
          
    def update_from_model(self, response: str):
        # Parse the response and update agent_data
        import re
        test_action = extract_test_cases(response)
        
        # Parse test cases
        self.current_action = test_action
   
        
        return self.current_action
    
    def calculate_reward(self, env_data: Env, mode: str = "sum") -> float:
        """
        Compute reward based on environment state, supporting three modes:
        - generated: use generated_pass_ratio (prefer generated_test_vs_generated_code_match_ratio, fallback to generated_test_vs_golden_code_match_ratio)
        - golden: use golden_pass_ratio (golden_test_vs_generated_code_match_ratio)
        - sum/both/others: sum of both
        """
        state = getattr(env_data, "state", None)
        generated_pass_ratio = 0.0
        golden_pass_ratio = 0.0

        if state is not None:
            # Generated tests vs generated code
            gen_vs_gen = getattr(state, "generated_test_vs_generated_code_match_ratio", None)
            # Generated tests vs golden code (as fallback)
            gold_vs_gen = getattr(state, "golden_test_vs_generated_code_match_ratio", None)

            if isinstance(gen_vs_gen, (int, float)):
                generated_pass_ratio = float(gen_vs_gen)
            if isinstance(gold_vs_gen, (int, float)):
                golden_pass_ratio = float(gold_vs_gen)

        m = (mode or "sum").lower()
        if m in ("generated", "gen"):
            reward = generated_pass_ratio
        elif m in ("golden", "gold"):
            reward = golden_pass_ratio
        else:
            reward = generated_pass_ratio + golden_pass_ratio

        # Record and return
        self.agent_reward = reward
        if self.info is None:
            self.info = {}
        self.info.update({
            "generated_pass_ratio": generated_pass_ratio,
            "golden_pass_ratio": golden_pass_ratio,
            "reward_mode": m,
        })
        
        return reward

    def reset(self):
        """
        Reset the agent's internal state for a new episode.
        """
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
        self.current_reward = None
        self.current_info = None
        self.current_action = None
        self.current_prompt = None
        self.current_response = None